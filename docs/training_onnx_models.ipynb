{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fine-Tuning ResNet with BlobDatabase\n",
    "\n",
    "In this tutorial, we will walk through the process of training a Deep Learning model to classify blobs using data stored in a `BlobDatabase`. \n",
    "\n",
    "### What we will cover:\n",
    "1.  **Concepts**: Understanding Transfer Learning and ResNet.\n",
    "2.  **Data Preparation**: Using `BlobDatabase` to create PyTorch Datasets with a proper train/validation split.\n",
    "3.  **Model Setup**: Modifying a pre-trained ResNet50 for our specific classes.\n",
    "4.  **Training**: The training loop with validation.\n",
    "5.  **Export**: Saving the model to ONNX format for deployment.\n",
    "\n",
    "### Prerequisites\n",
    "A Blob Database with reference classes assigned to blobs.\n",
    "\n",
    "And install the required libraries:\n",
    "```bash\n",
    "pip install videometer torch torchvision scikit-learn onnx onnxruntime\n",
    "```\n",
    "\n",
    "Notice that if you wish to use your GPU for training, then PyTorch might need to be installed from a different package source. [Consult the PyTorch website](https://pytorch.org/get-started/locally/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concepts: Why ResNet and Transfer Learning?\n",
    "\n",
    "**ResNet (Residual Network)** is a powerful convolutional neural network architecture that uses \"skip connections\" to allow gradients to flow easily during training. This enables the training of very deep networks (like the 50-layer version we will use) without the performance degrading.\n",
    "\n",
    "**Fine-Tuning (Transfer Learning)** is the technique of taking a model already trained on a massive dataset (like ImageNet, with 1.2 million images) and adapting it to a new, smaller task. \n",
    "\n",
    "* **Why?** The early layers of the network have already learned to detect edges, textures, and shapes. We only need to \"teach\" the final layers to recognize the specific difference between your classes. This requires far less data and training time than starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from videometer.BlobDatabase import BlobDatabase, BlobDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Splitting\n",
    "\n",
    "We need to create two separate datasets: one for **training** (where the model learns) and one for **validation** (where we check its progress).\n",
    "\n",
    "We will use a **robust splitting strategy**:\n",
    "1.  Extract the list of all blob IDs and their labels.\n",
    "2.  Shuffle and split these IDs.\n",
    "3.  Create two separate `BlobDataset` objects.\n",
    "4.  Apply **Data Augmentation** (random resizing, flipping) only to the training set to make the model robust.\n",
    "5.  Apply standard **Normalization** to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 26 samples across 3 classes.\n",
      "Classes: {'Double': 0, 'Large': 1, 'Small': 2}\n",
      "Found 26 samples.\n",
      "Classes: {0: 'Double', 1: 'Large', 2: 'Small'}\n",
      "Training count: 20\n",
      "Validation count: 6\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Database\n",
    "db_path = \"example.blobdb\"\n",
    "db = BlobDatabase(db_path)\n",
    "\n",
    "# 2. Get the raw list of samples (IDs and Labels) without loading images yet\n",
    "# We use a temporary dataset wrapper to fetch the index map efficiently\n",
    "temp_ds = db.get_dataset(target_class_type=\"reference\")\n",
    "all_samples = temp_ds.samples\n",
    "class_map = temp_ds.class_map\n",
    "num_classes = len(class_map)\n",
    "\n",
    "print(f\"Found {len(all_samples)} samples.\")\n",
    "print(f\"Classes: {class_map}\")\n",
    "\n",
    "# 3. Split IDs into Train (80%) and Validation (20%)\n",
    "train_samples, val_samples = train_test_split(all_samples, test_size=0.2, random_state=42, stratify=[s[1] for s in all_samples])\n",
    "\n",
    "print(f\"Training count: {len(train_samples)}\")\n",
    "print(f\"Validation count: {len(val_samples)}\")\n",
    "\n",
    "# 4. Define Transforms\n",
    "# ImageNet stats for normalization\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "# 5. Create Datasets using the factory constructor logic\n",
    "train_dataset = BlobDataset(db_path, train_samples, class_map, transform=train_transforms)\n",
    "val_dataset = BlobDataset(db_path, val_samples, class_map, transform=val_transforms)\n",
    "\n",
    "# 6. Create DataLoaders\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup\n",
    "\n",
    "We load a pre-trained **ResNet50**. \n",
    "The original model outputs 1000 classes (for ImageNet). We must replace the final fully connected layer (`fc`) to output `num_classes` (the number of classes in your database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Detect device (CUDA, MPS for Mac, or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Modify the final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move model to GPU/CPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "We define the logic to train the model. This includes:\n",
    "1.  **Forward Pass**: Compute predictions.\n",
    "2.  **Loss Calculation**: Compare prediction to truth (CrossEntropy).\n",
    "3.  **Backward Pass**: Calculate gradients.\n",
    "4.  **Optimizer Step**: Update weights.\n",
    "\n",
    "We save the best model weights based on validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train' and scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Deep copy the model if it's the best one so far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.9307 Acc: 0.5000\n",
      "val Loss: 0.9088 Acc: 0.6667\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.7000\n",
      "val Loss: 0.9531 Acc: 0.6667\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.3285 Acc: 0.9000\n",
      "val Loss: 0.5772 Acc: 0.6667\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.3514 Acc: 0.9000\n",
      "val Loss: 0.3763 Acc: 0.8333\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.1907 Acc: 0.9500\n",
      "val Loss: 0.2705 Acc: 0.8333\n",
      "\n",
      "Training complete in 4m 1s\n",
      "Best val Acc: 0.833333\n"
     ]
    }
   ],
   "source": [
    "# Setup Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# Using SGD with momentum is standard for ResNet fine-tuning\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Run Training\n",
    "trained_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export to ONNX\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is a standard format for representing machine learning models. Exporting to ONNX allows you to run this model in other languages (C++, C#, etc.) or on edge devices without needing Python or PyTorch installed.\n",
    "\n",
    "We specify `dynamic_axes` to allow the batch size to vary (e.g., process 1 image or 100 images at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heh\\AppData\\Local\\Temp\\ipykernel_31500\\2037365576.py:7: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to blob_resnet50.onnx\n"
     ]
    }
   ],
   "source": [
    "ONNX_PATH = \"blob_resnet50.onnx\"\n",
    "\n",
    "# 1. Create a dummy input matching the input size (1 batch, 3 channels, 224x224)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# 2. Export\n",
    "torch.onnx.export(\n",
    "    trained_model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},  # Variable length axes\n",
    "        'output': {0: 'batch_size'}\n",
    "    },\n",
    "    dynamo=False\n",
    ")\n",
    "print(f\"Model exported to {ONNX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify ONNX Model\n",
    "\n",
    "It is best practice to verify that the exported model produces the same results as the PyTorch model. We utilize `onnxruntime` for this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "# 1. Run inference with PyTorch\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    torch_out = trained_model(dummy_input)\n",
    "\n",
    "# 2. Run inference with ONNX Runtime\n",
    "ort_session = onnxruntime.InferenceSession(ONNX_PATH, providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# 3. Compare\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "print(\"✅ Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
